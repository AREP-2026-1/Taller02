{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04628e1f",
   "metadata": {},
   "source": [
    "# Heart Disease Logistic Regression Analysis\n",
    "\n",
    "This notebook performs:\n",
    "1. Data loading and target binarization\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Data preparation (train/test split, normalization)\n",
    "\n",
    "**Note:** No high-level ML libraries (scikit-learn, statsmodels, TensorFlow, PyTorch) are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4b473",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Heart_Disease_Prediction.csv')\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891403ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the target column: 1 = Presence (disease), 0 = Absence (no disease)\n",
    "df['Heart Disease Binary'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"Target column mapping:\")\n",
    "print(df[['Heart Disease', 'Heart Disease Binary']].drop_duplicates())\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['Heart Disease Binary'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2328792",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*60)\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method for numerical columns\n",
    "print(\"=\"*60)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression', 'Number of vessels fluro']\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"\\nOutliers per column:\")\n",
    "outlier_summary = []\n",
    "for col in numerical_cols:\n",
    "    n_outliers, lb, ub = detect_outliers_iqr(df, col)\n",
    "    outlier_summary.append({'Column': col, 'Outliers': n_outliers, 'Lower Bound': lb, 'Upper Bound': ub})\n",
    "    print(f\"  {col}: {n_outliers} outliers (bounds: [{lb:.2f}, {ub:.2f}])\")\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers by capping (winsorizing) at IQR bounds\n",
    "print(\"=\"*60)\n",
    "print(\"HANDLING OUTLIERS (Capping at IQR bounds)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers\n",
    "    original_outliers = ((df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)).sum()\n",
    "    df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    print(f\"  {col}: Capped {original_outliers} outliers\")\n",
    "\n",
    "print(\"\\nOutliers handled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts = df_clean['Heart Disease Binary'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[0].bar(['Absence (0)', 'Presence (1)'], [class_counts[0], class_counts[1]], color=colors)\n",
    "axes[0].set_xlabel('Heart Disease')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Class Distribution (Bar Chart)')\n",
    "for i, v in enumerate([class_counts[0], class_counts[1]]):\n",
    "    axes[0].text(i, v + 2, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie([class_counts[0], class_counts[1]], \n",
    "            labels=['Absence (0)', 'Presence (1)'], \n",
    "            autopct='%1.1f%%',\n",
    "            colors=colors,\n",
    "            explode=(0.02, 0.02))\n",
    "axes[1].set_title('Class Distribution (Pie Chart)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass 0 (Absence): {class_counts[0]} ({class_counts[0]/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Class 1 (Presence): {class_counts[1]} ({class_counts[1]/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f6526",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### Feature Selection (â‰¥6 features)\n",
    "Selected features:\n",
    "1. **Age** - Patient age\n",
    "2. **BP** - Blood Pressure\n",
    "3. **Cholesterol** - Cholesterol level\n",
    "4. **Max HR** - Maximum Heart Rate\n",
    "5. **ST depression** - ST depression induced by exercise\n",
    "6. **Number of vessels fluro** - Number of major vessels colored by fluoroscopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e60edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "selected_features = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression', 'Number of vessels fluro']\n",
    "\n",
    "X = df_clean[selected_features].values\n",
    "y = df_clean['Heart Disease Binary'].values\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSelected {len(selected_features)} features: {selected_features}\")\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Train/Test Split (70/30) - Manual implementation without sklearn\n",
    "def stratified_train_test_split(X, y, test_size=0.3, random_seed=42):\n",
    "    \"\"\"\n",
    "    Manual implementation of stratified train-test split.\n",
    "    Ensures both train and test sets have similar class proportions.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_0_indices = np.where(y == 0)[0]\n",
    "    class_1_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(class_0_indices)\n",
    "    np.random.shuffle(class_1_indices)\n",
    "    \n",
    "    # Calculate split points for each class\n",
    "    n_test_class_0 = int(len(class_0_indices) * test_size)\n",
    "    n_test_class_1 = int(len(class_1_indices) * test_size)\n",
    "    \n",
    "    # Split indices for each class\n",
    "    test_indices_0 = class_0_indices[:n_test_class_0]\n",
    "    train_indices_0 = class_0_indices[n_test_class_0:]\n",
    "    \n",
    "    test_indices_1 = class_1_indices[:n_test_class_1]\n",
    "    train_indices_1 = class_1_indices[n_test_class_1:]\n",
    "    \n",
    "    # Combine indices\n",
    "    train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
    "    test_indices = np.concatenate([test_indices_0, test_indices_1])\n",
    "    \n",
    "    # Shuffle combined indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Create train and test sets\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.3, random_seed=42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STRATIFIED TRAIN/TEST SPLIT (70/30)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"  Class 0: {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(f\"  Class 0: {np.sum(y_test == 0)} ({np.sum(y_test == 0)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y_test == 1)} ({np.sum(y_test == 1)/len(y_test)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
